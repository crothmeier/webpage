<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Learn how to secure healthcare LLM deployments with practical architecture patterns, GPU-level isolation, and comprehensive monitoring for PHI protection." />
  <title>Securing Healthcare LLMs: On-Prem Deployment Architecture for PHI Protection | Lazarus Laboratories Consulting</title>
  
  <!-- Google Analytics (GA4) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-L3XZ9E5JLY"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-L3XZ9E5JLY');
  </script>

  <!-- Bootstrap 5 CSS -->
  <link 
    href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"
    rel="stylesheet" 
    integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM"
    crossorigin="anonymous"
  >
  <link rel="stylesheet" href="styles.css" type="text/css">

  <!-- Schema markup for better SEO -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Securing Healthcare LLMs: On-Prem Deployment Architecture for PHI Protection",
    "datePublished": "2025-05-10",
    "author": {
      "@type": "Person",
      "name": "Christopher Rothmeier"
    },
    "publisher": {
      "@type": "Organization",
      "name": "Lazarus Laboratories Consulting",
      "logo": {
        "@type": "ImageObject",
        "url": "photos/lazlogo2-draft2.png"
      }
    },
    "description": "Learn how to secure healthcare LLM deployments with practical architecture patterns, GPU-level isolation, and comprehensive monitoring for PHI protection."
  }
  </script>
</head>

<body>
  <!-- Simple Navbar -->
  <nav class="navbar navbar-expand-lg navbar-light bg-light">
    <div class="container">
      <a class="navbar-brand d-flex align-items-center" href="index.html">
        <img src="photos/lazlogo2-draft2.png" alt="Lazarus Labs" height="30" class="me-2">
        <span>Lazarus Labs</span>
      </a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" 
              data-bs-target="#navbarNav" aria-controls="navbarNav" 
              aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
          <li class="nav-item"><a class="nav-link" href="blog.html">Blog</a></li>
          <li class="nav-item">
            <a class="nav-link" href="https://www.linkedin.com/in/christopher-rothmeier" target="_blank">LinkedIn</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  
  <!-- Blog Post Content Section -->
  <section class="py-5">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-8">
          <!-- Back link -->
          <div class="mb-4">
            <a href="blog.html" class="text-decoration-none">&larr; Back to Blog</a>
          </div>
          
          <!-- Blog Title and Meta -->
          <h1 class="blog-post-title mb-3">Securing Healthcare LLMs: On-Prem Deployment Architecture for PHI Protection</h1>
          <div class="blog-post-meta mb-4">
            By Christopher Rothmeier | May 10, 2025
          </div>
          
          <!-- Featured Image -->
          <div class="mb-4">
            <img 
              src="photos/datacenter.jpeg" 
              alt="Secure Healthcare Infrastructure" 
              class="img-fluid rounded" 
              loading="lazy"
              onerror="this.src='photos/glasses.jpeg';">
          </div>
          
          <!-- Blog Content -->
          <div class="blog-post-content">
            <p>
              Large Language Models (LLMs) are revolutionizing healthcare operations and clinical workflows, but their deployment introduces significant security and compliance challenges. Following my recent posts on <a href="blogvmware.html">VMware migration</a> and <a href="blog-gpu-security-2025.html">GPU infrastructure security</a>, I've received numerous questions about how these technologies can be applied specifically to secure healthcare AI deployments where Protected Health Information (PHI) is involved.
            </p>
            <p>
              With the FDA's new draft guidance on "AI-Enabled Device Software Functions" (January 2025) and the EU AI Act's healthcare provisions coming into force, healthcare organizations face a complex balancing act: harnessing LLMs' transformative potential while ensuring robust PHI protection and regulatory compliance.
            </p>
            <p>
              This post explores battle-tested architecture patterns for secure healthcare LLM deployments, drawing from my experience implementing on-premises AI infrastructure for several healthcare clients.
            </p>
            
            <h2>The Unique Challenges of Healthcare LLM Security</h2>
            <p>
              Healthcare LLM deployments face a trifecta of challenges that require specialized security approaches:
            </p>
            
            <h3>1. PHI Leakage Risks</h3>
            <p>
              The primary concern with healthcare LLMs is their potential to inadvertently expose Protected Health Information. This can occur through:
            </p>
            <ul>
              <li><strong>Model memorization:</strong> LLMs can memorize training data, potentially regurgitating PHI when prompted</li>
              <li><strong>Prompt injection attacks:</strong> Malicious prompts designed to extract sensitive information</li>
              <li><strong>Inference logs exposure:</strong> Logs containing PHI being stored insecurely or retained unnecessarily</li>
              <li><strong>Model weights extraction:</strong> Sophisticated attacks that can extract training data from model parameters</li>
            </ul>
            
            <h3>2. Regulatory Compliance Requirements</h3>
            <p>
              Healthcare LLM deployments must navigate an increasingly complex regulatory landscape:
            </p>
            <ul>
              <li><strong>HIPAA compliance:</strong> Requiring comprehensive safeguards for PHI throughout the LLM pipeline</li>
              <li><strong>EU AI Act (August 2024):</strong> Establishing stricter requirements for high-risk healthcare AI applications</li>
              <li><strong>FDA guidance (January 2025):</strong> Introducing Predetermined Change Control Plans (PCCP) for AI-enabled medical software</li>
              <li><strong>State-level regulations:</strong> Additional requirements that vary by jurisdiction</li>
            </ul>
            <p>
              Non-compliance penalties are substantial—up to €35 million or 7% of global turnover under the EU AI Act, and significant HIPAA violation penalties ranging from $100 to $50,000 per violation with an annual maximum of $1.5 million for identical violations.
            </p>
            
            <h3>3. Performance and Latency Constraints</h3>
            <p>
              Security cannot come at the expense of clinical usability. Healthcare LLMs must maintain performance thresholds while implementing robust security:
            </p>
            <ul>
              <li><strong>Clinical decision support:</strong> Requiring response times under 2 seconds</li>
              <li><strong>Real-time documentation:</strong> Needing at least 20 tokens/second generation throughput</li>
              <li><strong>High availability requirements:</strong> Ensuring system accessibility for critical care applications</li>
            </ul>
            
            <h2>Secure Architecture Blueprint for Healthcare LLMs</h2>
            <p>
              Based on my implementations across various healthcare environments, I've developed a layered security architecture that balances protection, compliance, and performance:
            </p>
            
            <h3>Layer 1: Physical Infrastructure and GPU-Level Isolation</h3>
            <p>
              The foundation of secure healthcare LLM deployment begins with robust physical infrastructure:
            </p>
            
            <h4>On-Premises GPU Infrastructure</h4>
            <p>
              For healthcare organizations handling sensitive PHI, an on-premises GPU infrastructure provides significant security advantages:
            </p>
            <ul>
              <li><strong>Physical access controls:</strong> Server racks with biometric authentication and tamper detection</li>
              <li><strong>Network segmentation:</strong> Dedicated physical networks for AI workloads separated from clinical systems</li>
              <li><strong>GPU-level isolation:</strong> Utilizing technologies like NVIDIA Multi-Instance GPU (MIG) to create hardware-level boundaries between workloads</li>
            </ul>
            <p>
              I've found that enterprise-grade hardware like NVIDIA A100/H100 GPUs with MIG capabilities provides the best balance of performance and isolation for healthcare LLM workloads. This approach allows for dedicated, isolated GPU resources for different applications (e.g., separating clinical decision support from coding assistance).
            </p>
            
            <h4>TPM-Based Security Enhancements</h4>
            <p>
              Modern server platforms offer hardware-based security features that should be leveraged:
            </p>
            <pre class="bg-light p-3 rounded">
# Example: Enabling TPM-based secure boot and measured boot
# Edit GRUB configuration
sudo nano /etc/default/grub

# Add secure boot parameters
GRUB_CMDLINE_LINUX="... tpm_tis.force=1 tpm_tis.interrupts=0"

# Update GRUB
sudo update-grub</pre>
            <p>
              For healthcare LLM deployments, I recommend:
            </p>
            <ul>
              <li><strong>Secure Boot:</strong> Ensuring only signed, verified code runs on the system</li>
              <li><strong>Measured Boot:</strong> Using TPM to verify system integrity at startup</li>
              <li><strong>Remote Attestation:</strong> Providing cryptographic proof that the system is in a known-good state</li>
              <li><strong>Encrypted model storage:</strong> Using TPM-sealed encryption keys for LLM weights</li>
            </ul>
            
            <h3>Layer 2: Isolation Strategies for PHI Protection</h3>
            <p>
              Based on my implementations and the evolving best practices, three proven approaches exist for isolating PHI from LLM systems:
            </p>
            
            <h4>1. Air-Gapped Architecture</h4>
            <p>
              The most secure approach for high-sensitivity applications:
            </p>
            <ul>
              <li><strong>Complete physical separation</strong> between LLM environments and PHI systems</li>
              <li><strong>Strictly controlled data transfer</strong> through audited channels</li>
              <li><strong>One-way data flows</strong> where possible to prevent PHI exfiltration</li>
            </ul>
            <p>
              While this approach offers maximum security, it does impact workflow efficiency and requires careful design to maintain usability.
            </p>
            
            <h4>2. RAG with Role-Based Access Control</h4>
            <p>
              A more balanced approach that maintains security while improving usability:
            </p>
            <pre class="bg-light p-3 rounded">
# Pseudocode for role-based RAG implementation
def retrieve_context(query, user_role, patient_id):
    # Verify user authorization for this patient
    if not is_authorized(user_role, patient_id, "read"):
        return []
    
    # Log access attempt with user context
    log_access_attempt(user_id, patient_id, "RAG_retrieval")
    
    # Apply role-based filters to embedding search
    role_filters = get_role_filters(user_role)
    
    # Retrieve only authorized embeddings
    embeddings = vector_store.search(
        query_embedding=embed(query),
        filters=role_filters,
        patient_id=patient_id
    )
    
    # Apply additional PHI minimization
    return apply_phi_minimization(embeddings)</pre>
            <p>
              In this model:
            </p>
            <ul>
              <li>LLMs access data only through permissioned vector stores with embedded access controls</li>
              <li>Authorization metadata is maintained for each embedding</li>
              <li>Contextual access policies enforce role-based restrictions</li>
              <li>All access is logged for audit purposes</li>
            </ul>
            
            <h4>3. Proxy-Based Architecture</h4>
            <p>
              A sophisticated approach that provides granular control:
            </p>
            <ul>
              <li>All LLM interactions pass through a security proxy layer</li>
              <li>Token-level analysis prevents PHI from passing into model prompts</li>
              <li>Dynamic PHI redaction in both inputs and outputs</li>
              <li>Comprehensive logging and alerting for potential PHI exposure</li>
            </ul>
            <p>
              For most healthcare implementations I've worked on, a combination of these approaches provides the optimal security posture. Critical clinical applications might use air-gapped systems, while administrative functions leverage proxy-based architectures for better workflow integration.
            </p>
            
            <h3>Layer 3: Container Hardening and Runtime Security</h3>
            <p>
              Modern healthcare LLM deployments typically leverage containerization for deployment flexibility, but this requires careful security hardening:
            </p>
            
            <h4>Non-Root Inference Services</h4>
            <p>
              Running LLM inference as non-privileged users significantly reduces the attack surface:
            </p>
            <pre class="bg-light p-3 rounded">
# Dockerfile example with security hardening
FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04 as base

# Create non-root user
RUN groupadd -g 1000 llmuser && \
    useradd -u 1000 -g llmuser -s /bin/bash llmuser

# Set up model directory with appropriate permissions
RUN mkdir -p /opt/models && \
    chown llmuser:llmuser /opt/models

# Install only necessary dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip && \
    rm -rf /var/lib/apt/lists/*

# Copy application code
COPY --chown=llmuser:llmuser app/ /app/

# Install Python dependencies
RUN pip3 install --no-cache-dir -r /app/requirements.txt

# Switch to non-root user
USER llmuser

# Use read-only filesystem where possible
VOLUME ["/opt/models:ro", "/app/config:ro"]

# Set secure environment
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

# Run with minimal capabilities
ENTRYPOINT ["python3", "/app/inference_server.py"]</pre>
            <p>
              Key container security practices for healthcare LLMs include:
            </p>
            <ul>
              <li><strong>Non-root operation:</strong> Running all services as unprivileged users</li>
              <li><strong>Read-only filesystems:</strong> Preventing runtime modifications to model files</li>
              <li><strong>Minimal base images:</strong> Reducing the attack surface by including only necessary components</li>
              <li><strong>Content trust:</strong> Verifying image signatures before deployment</li>
              <li><strong>Runtime vulnerability scanning:</strong> Continuous monitoring for newly discovered vulnerabilities</li>
            </ul>
            <p>
              For healthcare environments, I always recommend implementing these additional container security measures:
            </p>
            <ul>
              <li><strong>Seccomp profiles:</strong> Restricting the system calls that containers can make</li>
              <li><strong>AppArmor/SELinux policies:</strong> Implementing mandatory access controls</li>
              <li><strong>Network policy enforcement:</strong> Limiting container communications to only required services</li>
              <li><strong>Resource limitations:</strong> Preventing resource exhaustion attacks</li>
            </ul>
            
            <h3>Layer 4: Comprehensive Observability and Audit</h3>
            <p>
              Security without visibility is incomplete. Healthcare LLM deployments require robust monitoring:
            </p>
            
            <h4>Prometheus-Based Inference Auditing</h4>
            <p>
              Implementing a comprehensive monitoring stack provides both security insights and operational visibility:
            </p>
            <pre class="bg-light p-3 rounded">
# Prometheus metrics for LLM inference auditing
from prometheus_client import Counter, Histogram, Info

# Track overall request patterns
inference_requests = Counter(
    'llm_inference_requests_total', 
    'Total number of inference requests',
    ['model', 'application', 'user_role']
)

# Track inference time
inference_latency = Histogram(
    'llm_inference_duration_seconds',
    'Time spent processing inference requests',
    ['model', 'application'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

# Track potential PHI exposure attempts
phi_detection_events = Counter(
    'llm_phi_detection_events_total',
    'Number of potential PHI exposure events detected',
    ['severity', 'type', 'action_taken']
)

# Model information
model_info = Info('llm_model', 'Information about the deployed model')
model_info.info({
    'name': 'clinical-bert-7b',
    'version': '1.2.3',
    'last_updated': '2025-05-01',
    'training_governance_id': 'TR-2025-042'
})</pre>
            <p>
              A comprehensive observability layer should include:
            </p>
            <ul>
              <li><strong>Inference logging:</strong> Detailed logs of model inputs and outputs (with PHI redaction)</li>
              <li><strong>Performance metrics:</strong> Tracking latency, throughput, and resource utilization</li>
              <li><strong>Security events:</strong> Monitoring for anomalous access patterns or potential attacks</li>
              <li><strong>Compliance dashboards:</strong> Real-time visibility into regulatory metrics</li>
              <li><strong>Alerting:</strong> Immediate notification of potential security incidents</li>
            </ul>
            <p>
              For one healthcare client, we implemented a specialized monitoring dashboard that tracked:
            </p>
            <ul>
              <li>Potential PHI inclusion attempts in prompts</li>
              <li>Unusual access patterns by role and department</li>
              <li>Model confidence scores to flag potentially hallucinated outputs</li>
              <li>Response latency to ensure clinical workflow requirements were met</li>
            </ul>
            <p>
              This visibility not only enhanced security but also provided valuable insights for model optimization and compliance reporting.
            </p>
            
            <h2>Practical Implementation: Mayo Clinic's Approach</h2>
            <p>
              Healthcare organizations can learn from Mayo Clinic's phased LLM implementation strategy, which balances innovation with appropriate safeguards:
            </p>
            
            <h3>Phase 1: Administrative Applications</h3>
            <p>
              Beginning with lower-risk, high-value applications:
            </p>
            <ul>
              <li><strong>Medical coding assistance:</strong> Using LLMs to suggest appropriate coding based on documentation</li>
              <li><strong>Documentation summarization:</strong> Creating structured summaries of unstructured notes</li>
              <li><strong>Patient communication drafting:</strong> Generating initial drafts of patient instructions</li>
            </ul>
            <p>
              These applications demonstrated value while minimizing risk, building organizational confidence in the technology.
            </p>
            
            <h3>Phase 2: Clinical Support Tools</h3>
            <p>
              Progressing to clinical applications with appropriate guardrails:
            </p>
            <ul>
              <li><strong>Differential diagnosis support:</strong> Suggesting possible diagnoses based on symptoms and patient history</li>
              <li><strong>Literature search:</strong> Finding relevant research for specific clinical scenarios</li>
              <li><strong>SDOH extraction:</strong> Identifying social determinants of health from clinical notes</li>
            </ul>
            <p>
              Mayo's implementation of these tools helped identify 93.8% of patients with adverse social determinants compared to 2% through traditional coding.
            </p>
            
            <h3>Phase 3: Integrated Clinical Workflows</h3>
            <p>
              The final phase integrated LLMs directly into clinical workflows:
            </p>
            <ul>
              <li><strong>EHR integration:</strong> Embedding LLM capabilities within the existing clinical systems</li>
              <li><strong>Clinician-LLM collaboration:</strong> Creating interfaces that facilitate human-AI teamwork</li>
              <li><strong>Continuous learning:</strong> Implementing feedback loops to improve model performance</li>
            </ul>
            <p>
              Throughout all phases, Mayo Clinic maintained a dedicated AI oversight committee with representation from clinical, technical, legal, and ethics departments. This governance structure ensured appropriate safeguards while enabling innovation.
            </p>
            
            <h2>Beyond Technical Controls: Governance Framework</h2>
            <p>
              Securing healthcare LLMs extends beyond technical controls to include robust governance:
            </p>
            
            <h3>Cross-Functional Committee</h3>
            <p>
              Effective governance requires structured oversight with clear roles and responsibilities:
            </p>
            <ul>
              <li><strong>Clinical representation:</strong> Ensuring patient safety and clinical workflow considerations</li>
              <li><strong>Technical expertise:</strong> Providing implementation guidance and security oversight</li>
              <li><strong>Legal/compliance:</strong> Navigating the complex regulatory landscape</li>
              <li><strong>Ethics:</strong> Addressing the ethical implications of AI in healthcare</li>
              <li><strong>Patient advocacy:</strong> Representing patient interests and concerns</li>
            </ul>
            
            <h3>Use Case Classification Framework</h3>
            <p>
              Implementing a tiered approach to LLM applications based on risk:
            </p>
            <ul>
              <li><strong>Low risk:</strong> Administrative applications with minimal PHI exposure</li>
              <li><strong>Medium risk:</strong> Clinical documentation and indirect patient care</li>
              <li><strong>High risk:</strong> Direct patient care and clinical decision support</li>
            </ul>
            <p>
              Each risk tier should have corresponding security requirements, validation processes, and human oversight mechanisms.
            </p>
            
            <h3>Human-in-the-Loop Design</h3>
            <p>
              All clinical LLM applications should follow human-in-the-loop design principles:
            </p>
            <ul>
              <li><strong>Physician as final authority:</strong> Ensuring clinicians make the ultimate decisions</li>
              <li><strong>Transparent attribution:</strong> Clearly distinguishing between model-generated and human-authored content</li>
              <li><strong>Override mechanisms:</strong> Allowing clinicians to easily correct or disregard AI suggestions</li>
              <li><strong>Feedback loops:</strong> Capturing clinician input to improve model performance</li>
            </ul>
            
            <h2>The Future of Secure Healthcare LLMs</h2>
            <p>
              Looking ahead, several emerging trends will shape healthcare LLM security:
            </p>
            
            <h3>Multimodal Capabilities</h3>
            <p>
              The integration of text with medical imaging and other clinical data types will require enhanced security approaches:
            </p>
            <ul>
              <li><strong>Multi-layer PHI detection:</strong> Identifying protected information across text, images, and structured data</li>
              <li><strong>Cross-modal security:</strong> Ensuring PHI cannot leak between different data modalities</li>
              <li><strong>Enhanced privacy-preserving techniques:</strong> Developing new methods for secure multimodal analysis</li>
            </ul>
            
            <h3>Federated Learning</h3>
            <p>
              Decentralized model training offers promising privacy benefits:
            </p>
            <ul>
              <li><strong>Training across institutions:</strong> Improving models without sharing sensitive data</li>
              <li><strong>Local data processing:</strong> Keeping PHI within organizational boundaries</li>
              <li><strong>Differential privacy:</strong> Adding noise to protect individual patient data</li>
            </ul>
            
            <h3>Edge Deployment</h3>
            <p>
              Moving inference closer to the point of care:
            </p>
            <ul>
              <li><strong>On-device inference:</strong> Processing sensitive requests entirely on local hardware</li>
              <li><strong>Hybrid routing:</strong> Directing PHI-related queries to local models and general queries to cloud services</li>
              <li><strong>Progressive disclosure:</strong> Minimizing data exposure based on query requirements</li>
            </ul>
            
            <h2>Conclusion: A Balanced Approach</h2>
            <p>
              Securing healthcare LLM deployments requires a thoughtful balance between innovation and protection. By implementing a layered security architecture that includes physical isolation, container hardening, comprehensive monitoring, and robust governance, healthcare organizations can harness the transformative potential of LLMs while safeguarding patient information.
            </p>
            <p>
              The approach I've outlined—built on my experience implementing secure GPU infrastructure for both traditional and AI workloads—provides a practical framework for healthcare organizations navigating this complex landscape. As these technologies continue to evolve, maintaining this security-first mindset will be essential for responsible AI adoption in healthcare.
            </p>
            <p>
              At Lazarus Laboratories, we're committed to helping healthcare organizations implement secure, compliant LLM solutions that enhance patient care while protecting sensitive information. If you're considering deploying LLMs in a healthcare environment and want to discuss security architecture or implementation strategies, I'd be happy to share additional insights based on our experience.
            </p>
          </div>
          
          <!-- Post Navigation Section -->
          <div class="blog-post-navigation mt-5 pt-4 border-top">
            <div class="row">
              <div class="col-6 text-start">
                <a href="blogvmware.html" class="btn btn-outline-dark">
                  &larr; Previous Post<br>
                  <small>VMware to Proxmox Migration</small>
                </a>
              </div>
              <div class="col-6 text-end">
                <a href="blog4.html" class="btn btn-outline-dark">
                  Next Post &rarr;<br>
                  <small>GPU Virtualization vs. PCIe Passthrough</small>
                </a>
              </div>
            </div>
          </div>
          
          <!-- Related Posts Section -->
          <div class="related-posts mt-5">
            <h3 class="h4 mb-4">Related Articles</h3>
            <div class="row">
              <div class="col-md-6 mb-3">
                <div class="card h-100">
                  <div class="card-body">
                    <h4 class="h6 card-title">VMware to Proxmox Migration</h4>
                    <p class="card-text small">Explore strategies for migrating from VMware to Proxmox/KVM following the Broadcom acquisition.</p>
                    <a href="blogvmware.html" class="btn btn-sm btn-dark">Read More</a>
                  </div>
                </div>
              </div>
              <div class="col-md-6 mb-3">
                <div class="card h-100">
                  <div class="card-body">
                    <h4 class="h6 card-title">GPU Infrastructure Security</h4>
                    <p class="card-text small">Best practices for ensuring robust on-premises GPU deployments in 2025.</p>
                    <a href="blog-gpu-security-2025.html" class="btn btn-sm btn-dark">Read More</a>
                  </div>
                </div>
              </div>
            </div>
          </div>
          
          <!-- Author Bio -->
          <div class="mt-5 p-4 bg-light rounded">
            <div class="d-flex align-items-center mb-3">
              <div>
                <h5 class="mb-1">About the Author</h5>
                <p class="mb-0">
                  Christopher Rothmeier runs Lazarus Laboratories Consulting, specializing 
                  in hybrid cloud and AI-focused infrastructure. He's recently built an 
                  on-prem GPU lab to cut down on monthly cloud expenses—research that also 
                  fuels his search for a sysadmin role in Philadelphia. Connect on
                  <a 
                    href="https://www.linkedin.com/in/christopher-rothmeier"
                    target="_blank"
                  >
                    LinkedIn
                  </a>.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <!-- Share This Article Section -->
  <section class="py-4 bg-light">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-8 text-center">
          <h4 class="h5 mb-3">Share This Article</h4>
          <div class="d-flex justify-content-center gap-3">
            <a href="#" class="btn btn-outline-dark btn-sm">
              <i class="bi bi-linkedin"></i> LinkedIn
            </a>
            <a href="#" class="btn btn-outline-dark btn-sm">
              <i class="bi bi-twitter-x"></i> Twitter
            </a>
            <a href="#" class="btn btn-outline-dark btn-sm">
              <i class="bi bi-envelope"></i> Email
            </a>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <!-- CTA / CONTACT SECTION -->
  <section class="py-5 bg-light">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-8 text-center">
          <h3 class="mb-4">Questions about securing healthcare LLM deployments?</h3>
          <p class="mb-4">
            Feel free to reach out if you want to discuss secure AI implementation strategies 
            for your healthcare organization, or if you're looking for consulting in the Philadelphia area.
          </p>
          <a href="mailto:crothmeier@lazarus-labs.com" class="btn btn-primary">Contact Me</a>
        </div>
      </div>
    </div>
  </section>

  <!-- FOOTER -->
  <footer class="bg-light py-4 mt-5">
    <div class="container text-center">
      <p class="mb-0">© 2025 Lazarus Laboratories, LLC | 
        <a href="https://www.linkedin.com/in/christopher-rothmeier" target="_blank">
          Connect on LinkedIn
        </a>
      </p>
      <p class="mb-0">
        <small>Philadelphia, PA | Seeking Sysadmin Roles</small>
      </p>
    </div>
  </footer>

  <!-- Bootstrap Icons & JS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.1/font/bootstrap-icons.css">
  <script 
    src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-geWF76RCwLtnZ8qwWowPQNguL3RmwHVBC9FhGdlKrxdiJJigb/j/68SIy3Te4Bkz"
    crossorigin="anonymous">
  </script>
</body>
</html>